{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "#from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "# mobilenetv2\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_path = 'C:\\\\tmp\\\\CatDogsData\\\\Dog\\\\*.jpg'\n",
    "cat_path = 'C:\\\\tmp\\\\CatDogsData\\\\Cat\\\\*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "addrsd = glob.glob(dog_path)\n",
    "addrsc = glob.glob(cat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsd = [1 for addr in addrsd]  # 1 = dog, 0 =  cat\n",
    "labelsc = [0 for addr in addrsc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datad = []\n",
    "for imagePath in addrsd:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datad.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datac = []\n",
    "for imagePath in addrsc:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datac.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to shuffle data\n",
    "shuffle_data = True\n",
    "if shuffle_data:\n",
    "    d = list(zip(datad, labelsd))\n",
    "    c = list(zip(datac, labelsc))\n",
    "    e = d + c\n",
    "    shuffle(e)\n",
    "    data, labels = zip(*e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datad\n",
    "del datac\n",
    "del addrsd\n",
    "del addrsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(labels)\n",
    "X_train = np.array(data, dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for Resnet- 50\n",
    "X_train =  preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two inputs one each - left and right image\n",
    "left_input = Input((224,224,3))\n",
    "right_input = Input((224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "#Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.\n",
    "\n",
    "'''\n",
    "Please note that it’s usually better to intialize the layers with imagenet initializations than random. \n",
    "While training I will be updating the weights for each layer in each epoch. \n",
    "We don’t want to confuse this activity with transfer learning as \n",
    "I am not freezing any layer but initilializing each layer with imagenet weights\n",
    "'''\n",
    "convnet = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final fully connected layers\n",
    "x = convnet.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "preds = Dense(18, activation='sigmoid')(x) # Apply sigmoid\n",
    "convnet = Model(inputs=convnet.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying above model for both the left and right images\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidian Distance between the two images or encodings through the Resnet-50 architecture\n",
    "Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
    "\n",
    "# use and add the distance function\n",
    "Euc_distance = Euc_layer([encoded_l, encoded_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the prediction\n",
    "prediction = Dense(1,activation='sigmoid')(Euc_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the network with the left and right inputs and the ouput prediction\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the optimizer. Here I have used SGD with nesterov momentum\n",
    "optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network using binary cross entropy loss and the above optimizer\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = X_train[:180]\n",
    "label_list = Y_train[:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = []\n",
    "right_input = []\n",
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of pairs per image\n",
    "pairs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataset to train on\n",
    "for i in range(len(label_list)):\n",
    "    for j in range(pairs):\n",
    "        \n",
    "        # we need to make sure that we are not comparing with the same image\n",
    "        compare_to = i\n",
    "        \n",
    "        while compare_to == i: \n",
    "            compare_to = random.randint(0,179)\n",
    "        \n",
    "        left_input.append(image_list[i])\n",
    "        right_input.append(image_list[compare_to])\n",
    "        \n",
    "        if label_list[i] == label_list[compare_to]:\n",
    "            # if the images are same then label - 1\n",
    "            targets.append(1.)\n",
    "        else:\n",
    "            # if the images are different then label - 0\n",
    "            targets.append(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove single-dimensional entries from the shape of the arrays and making them ready to create the train & datasets \n",
    " \n",
    "#the train data - left right images arrays and target label\n",
    "left_input = np.squeeze(np.array(left_input))\n",
    "right_input = np.squeeze(np.array(right_input))\n",
    "targets = np.squeeze(np.array(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test datasets - left, right images and target label\n",
    "dog_image = X_train[4] #dog_image = 1, cat_image = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left = []\n",
    "test_right = []\n",
    "test_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_train)-180):\n",
    "    test_left.append(dog_image)\n",
    "    test_right.append(X_train[i+180])\n",
    "    test_targets.append(Y_train[i+180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left = np.squeeze(np.array(test_left))\n",
    "test_right = np.squeeze(np.array(test_right))\n",
    "test_targets = np.squeeze(np.array(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#from keras_input_pipeline import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "#config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 18)           126367634   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 18)           0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            19          lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 126,367,653\n",
      "Trainable params: 126,314,533\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1440 samples, validate on 20 samples\n",
      "Epoch 1/30\n",
      "1440/1440 [==============================] - 1104s 767ms/step - loss: 0.6884 - accuracy: 0.5646 - val_loss: 0.8070 - val_accuracy: 0.2000\n",
      "Epoch 2/30\n",
      "1440/1440 [==============================] - 1085s 753ms/step - loss: 0.4936 - accuracy: 0.8208 - val_loss: 0.7495 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "1440/1440 [==============================] - 1103s 766ms/step - loss: 0.4110 - accuracy: 0.9035 - val_loss: 0.7327 - val_accuracy: 0.5500\n",
      "Epoch 4/30\n",
      "1440/1440 [==============================] - 1091s 757ms/step - loss: 0.3598 - accuracy: 0.9458 - val_loss: 0.7337 - val_accuracy: 0.5500\n",
      "Epoch 5/30\n",
      "1440/1440 [==============================] - 1091s 757ms/step - loss: 0.3282 - accuracy: 0.9639 - val_loss: 0.7092 - val_accuracy: 0.5500\n",
      "Epoch 6/30\n",
      "1440/1440 [==============================] - 1092s 758ms/step - loss: 0.3067 - accuracy: 0.9694 - val_loss: 0.4564 - val_accuracy: 0.9000\n",
      "Epoch 7/30\n",
      "1440/1440 [==============================] - 1081s 751ms/step - loss: 0.2872 - accuracy: 0.9764 - val_loss: 0.3924 - val_accuracy: 0.9500\n",
      "Epoch 8/30\n",
      "1440/1440 [==============================] - 1091s 757ms/step - loss: 0.2711 - accuracy: 0.9840 - val_loss: 0.3802 - val_accuracy: 0.9500\n",
      "Epoch 9/30\n",
      "1440/1440 [==============================] - 1082s 751ms/step - loss: 0.2598 - accuracy: 0.9854 - val_loss: 0.3752 - val_accuracy: 0.9500\n",
      "Epoch 10/30\n",
      "1440/1440 [==============================] - 1084s 753ms/step - loss: 0.2482 - accuracy: 0.9882 - val_loss: 0.3688 - val_accuracy: 0.9500\n",
      "Epoch 11/30\n",
      "1440/1440 [==============================] - 1078s 749ms/step - loss: 0.2348 - accuracy: 0.9931 - val_loss: 0.3572 - val_accuracy: 0.9500\n",
      "Epoch 12/30\n",
      "1440/1440 [==============================] - 1080s 750ms/step - loss: 0.2233 - accuracy: 0.9931 - val_loss: 0.3526 - val_accuracy: 0.9500\n",
      "Epoch 13/30\n",
      "1440/1440 [==============================] - 1079s 749ms/step - loss: 0.2153 - accuracy: 0.9972 - val_loss: 0.3491 - val_accuracy: 0.9500\n",
      "Epoch 14/30\n",
      "1440/1440 [==============================] - 1079s 749ms/step - loss: 0.2099 - accuracy: 0.9972 - val_loss: 0.3455 - val_accuracy: 0.9500\n",
      "Epoch 15/30\n",
      "1440/1440 [==============================] - 1087s 755ms/step - loss: 0.2045 - accuracy: 0.9965 - val_loss: 0.3395 - val_accuracy: 0.9500\n",
      "Epoch 16/30\n",
      "1440/1440 [==============================] - 1082s 751ms/step - loss: 0.2007 - accuracy: 0.9979 - val_loss: 0.3320 - val_accuracy: 0.9500\n",
      "Epoch 17/30\n",
      "1440/1440 [==============================] - 1079s 750ms/step - loss: 0.1982 - accuracy: 0.9979 - val_loss: 0.3278 - val_accuracy: 0.9500\n",
      "Epoch 18/30\n",
      "1440/1440 [==============================] - 1082s 751ms/step - loss: 0.1946 - accuracy: 0.9979 - val_loss: 0.3209 - val_accuracy: 0.9500\n",
      "Epoch 19/30\n",
      "1440/1440 [==============================] - 1078s 749ms/step - loss: 0.1921 - accuracy: 0.9993 - val_loss: 0.3144 - val_accuracy: 0.9500\n",
      "Epoch 20/30\n",
      "1440/1440 [==============================] - 1081s 751ms/step - loss: 0.1883 - accuracy: 0.9986 - val_loss: 0.3090 - val_accuracy: 0.9500\n",
      "Epoch 21/30\n",
      "1440/1440 [==============================] - 1081s 751ms/step - loss: 0.1858 - accuracy: 0.9993 - val_loss: 0.3043 - val_accuracy: 0.9500\n",
      "Epoch 22/30\n",
      "1440/1440 [==============================] - 1083s 752ms/step - loss: 0.1830 - accuracy: 0.9986 - val_loss: 0.2973 - val_accuracy: 0.9500\n",
      "Epoch 23/30\n",
      "1440/1440 [==============================] - 1077s 748ms/step - loss: 0.1806 - accuracy: 0.9986 - val_loss: 0.2940 - val_accuracy: 0.9500\n",
      "Epoch 24/30\n",
      "1440/1440 [==============================] - 1082s 751ms/step - loss: 0.1786 - accuracy: 0.9993 - val_loss: 0.2896 - val_accuracy: 0.9500\n",
      "Epoch 25/30\n",
      "1440/1440 [==============================] - 1078s 748ms/step - loss: 0.1765 - accuracy: 0.9993 - val_loss: 0.2866 - val_accuracy: 0.9500\n",
      "Epoch 26/30\n",
      "1440/1440 [==============================] - 1084s 753ms/step - loss: 0.1745 - accuracy: 1.0000 - val_loss: 0.2839 - val_accuracy: 0.9500\n",
      "Epoch 27/30\n",
      "1440/1440 [==============================] - 1083s 752ms/step - loss: 0.1732 - accuracy: 0.9993 - val_loss: 0.2814 - val_accuracy: 0.9500\n",
      "Epoch 28/30\n",
      "1440/1440 [==============================] - 1080s 750ms/step - loss: 0.1715 - accuracy: 0.9986 - val_loss: 0.2799 - val_accuracy: 0.9500\n",
      "Epoch 29/30\n",
      "1440/1440 [==============================] - 1079s 749ms/step - loss: 0.1705 - accuracy: 0.9993 - val_loss: 0.2765 - val_accuracy: 0.9500\n",
      "Epoch 30/30\n",
      "1440/1440 [==============================] - 1077s 748ms/step - loss: 0.1692 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "    siamese_net.fit([left_input,right_input], targets,\n",
    "          batch_size=16,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          validation_data=([test_left,test_right],test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x20e83e82148>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss, acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save_weights('C:\\Master\\A7medMasterThesis\\weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 18)           126367634   input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 18)           0           model_4[1][0]                    \n",
      "                                                                 model_4[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            19          lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 126,367,653\n",
      "Trainable params: 126,314,533\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Two inputs one each - left and right image\n",
    "left_input = Input((224,224,3))\n",
    "right_input = Input((224,224,3))\n",
    "\n",
    "convnet = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Add the final fully connected layers\n",
    "x = convnet.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "preds = Dense(18, activation='sigmoid')(x) # Apply sigmoid\n",
    "convnet = Model(inputs=convnet.input, outputs=preds)\n",
    "\n",
    "#Applying above model for both the left and right images\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# Euclidian Distance between the two images or encodings through the Resnet-50 architecture\n",
    "Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
    "\n",
    "# use and add the distance function\n",
    "Euc_distance = Euc_layer([encoded_l, encoded_r])\n",
    "\n",
    "#identify the prediction\n",
    "prediction = Dense(1,activation='sigmoid')(Euc_distance)\n",
    "\n",
    "evaluateModel = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "#define the optimizer. Here I have used SGD with nesterov momentum\n",
    "optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "evaluateModel.compile(loss=\"binary_crossentropy\", optimizer=optim,metrics=['accuracy'])\n",
    "evaluateModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel.load_weights('C:\\Master\\A7medMasterThesis\\weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,acc = evaluateModel.evaluate([test_left,test_right], test_targets , verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949999988079071"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2752841114997864"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
